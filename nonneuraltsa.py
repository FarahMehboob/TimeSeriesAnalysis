# -*- coding: utf-8 -*-
"""NonNeuralTSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ExR2S8KhYgvI_HtXfAE9mhGEs738hW5

# Installations
"""

!pip install -U darts ##  installing libraries for TimeSeries Analysis
!pip install sklearn
!pip install pycatch22 #library for creating features

# Commented out IPython magic to ensure Python compatibility.
################################################################################

##  importing library for TimeSeries Analysis
from darts.dataprocessing.transformers import Scaler  ##  for scaling data
from darts import TimeSeries, concatenate ##  for datatype conversion and creating covariates
from darts.metrics import mape, mase, smape, rmse, rmsle ##  for computing error
from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis ## for creating cluster using darts features
from darts.models import (
    ARIMA,
    VARIMA,
    AutoARIMA,
    StatsForecastAutoARIMA,
    ExponentialSmoothing,
    StatsForecastAutoETS,
    StatsForecastAutoCES,
    BATS,
    TBATS,
    Theta,
    FourTheta,
    StatsForecastAutoTheta,
    Prophet,
    FFT,
    KalmanForecaster,
    Croston,
    RegressionModel,
    RandomForest,
    LinearRegressionModel,
    LightGBMModel,
    CatBoostModel,
    XGBModel,
    RNNModel,
    BlockRNNModel,
    NBEATSModel,
    NHiTSModel,
    TCNModel,
    TransformerModel,
    TFTModel,
    DLinearModel,
    NLinearModel,
    NaiveDrift,
    NaiveEnsembleModel,
    NaiveMean,
    NaiveMovingAverage,
    NaiveSeasonal
)

################################################################################
 ##  importing library for data and numerical processing
import numpy as np
import pandas as pd

################################################################################
##  importing library for Visualization
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
import logging
pd.set_option("display.precision",2)
np.set_printoptions(precision=2, suppress=True)
pd.options.display.float_format = '{:,.2f}'.format


################################################################################
## Importing Libraries for clustering and normalization

import sklearn
from sklearn import preprocessing

################################################################################
import sys
import time
from datetime import datetime
from functools import reduce

################################################################################
# Mounting Google Drive for accessing data files
################################################################################
from google.colab import drive
drive.mount('/content/drive')

"""# Inputs"""

indNum = int(19)  ## series number to predict.
clusterNum = 5  ## To Do: use sklearn clusters and take silhoutte score
doc_dir = '/content/gdrive/My Drive//NTSA-project/NonNeuralTSA/Models'

clusterFile = "/content/drive/MyDrive/NTSA-project/Notebooks-New/Dataset/raw.csv"
testFile =  "/content/drive/MyDrive/NTSA-project/Notebooks-New/Dataset/Weekly-test.csv"
trainFile =  "/content/drive/MyDrive/NTSA-project/Notebooks-New/Dataset/Weekly-train.csv"
metaData = "/content/drive/MyDrive/NTSA-project/Notebooks-New/Dataset/m4_info.csv"

models = [

    RegressionModel,
    LinearRegressionModel,
    LightGBMModel, #tree based model
    RandomForest #tree based model

   ]

################################################################################
#getData fetches data files and converts into pandas dataframe
################################################################################

def getData(testFile, trainFile, metaData):

  weeklytest = testFile
  weeklytrain = trainFile
  #m4_info = "/content/drive/MyDrive/NTSA-project/Notebooks-New/Dataset/m4_info.csv"

  m4_info = metaData
  dfinfo = pd.read_csv(m4_info)
  dfinfo = dfinfo.rename(columns={'M4id': 'V1'})
  dfinfo.head()

  #M4  Data
  wfytrain = pd.read_csv(weeklytrain)
  wfytest = pd.read_csv(weeklytest)


  #backup of original data
  wfytrain0 = wfytrain.copy()
  wfytest0 = wfytest.copy()

  return wfytest, wfytrain, dfinfo


################################################################################
#Merges timeseries data with respective timestamps and deletes non numerical columns
# The data is merged on index number [column V1]

################################################################################

def mergeData(wfytest, wfytrain, dfinfo):

  merged_test = pd.merge(dfinfo, wfytest, on='V1')
  merged_train = pd.merge(dfinfo, wfytrain, on='V1')
  DateTrain = merged_train['StartingDate']
  DateTest = merged_test['StartingDate']
  Date = merged_train['StartingDate']

  xdat_m4 = merged_train.drop([ 'Frequency', 'Horizon', 'SP','category', 'StartingDate'], axis =1)
  xdaty_m4 = merged_test.drop([ 'Frequency', 'Horizon', 'SP' , 'category', 'StartingDate'], axis =1)


  xdat_m4.drop('V1', axis = 1, inplace = True)
  xdaty_m4.drop('V1', axis = 1, inplace = True)

  return  xdat_m4, xdaty_m4, merged_test, merged_train, DateTrain, DateTest, Date



########################################################################
## used Kmeans to cluster the data
########################################################################

def makeCluster (numClusters, trainData, method):

  from sklearn.cluster import KMeans
  clusterraw = trainData
  kmeans = KMeans(n_clusters = numClusters, random_state=0).fit(~np.isnan(clusterraw))
  clusterraw[method] = kmeans.labels_
  clusterraw.to_csv('raw.csv')

  return clusterraw

########################################################################
## Groups all the timeseries from the same group as the forecasted TimeSeries
########################################################################

def clusterGrp(covd, numClusters, grp):

  a =[]
  for i in range(numClusters):
    locals()["class"+str(i)] = covd[covd[grp].isin([i])]
    a.append(locals()["class"+str(i)] )

  return a

######################################################################

def getClusterforScaling(clusterNum, grp22all):

  val = ""
  for i in range(clusterNum):
    a = grp22all[i].index == indNum
    if a.any():
      val = i

  return (val)

######################################################################
### Cluster Length is used to select all time series of the same length for covariates
######################################################################

def getClusterLength(clusterLength):

  clusterLength["Length"] = clusterLength.count(axis=1)
  clusterLength["TrueVal"] = clusterLength.index == indNum
  val = clusterLength.loc[clusterLength['TrueVal'] == True, 'TrueVal']
  length = clusterLength['Length'].loc[val.index]

  return (clusterLength, length)

######################################################################
# Do it: same length series are filtered got to explore resampling for fixing this
def pruneCluster(Cluster,clusterLength, grp):

  Cluster_new = Cluster[Cluster['Length'].isin([int(clusterLength.values)])]
  Cluster_new = Cluster_new.drop(columns = ['TrueVal','Length', grp], axis = 1)
  Cluster_new = Cluster_new.iloc[:, :int(clusterLength.values-2)]

  return(Cluster_new)

######################################################################
def min_Max(grprune):

  grprune = grprune.values()
  maxVal = grprune.max()
  minVal = grprune.min()
  diffVal =(maxVal-minVal)
  if diffVal != 0:
    grscaled = ((grprune - minVal)/ diffVal)
  grscaled = TimeSeries.from_values(grscaled)

  return grscaled

 ######################################################################
def min_MaxC0(grprune):

  maxVal = grprune.max()
  maxVal = maxVal
  minVal = grprune.min()
  minVal = minVal
  diffVal =(maxVal-minVal)
  grscaled = ((grprune - minVal)/ diffVal)

  return grscaled
 ######################################################################
def SkLearnMinMax(original):

  from sklearn.preprocessing import  MinMaxScaler
  original = original.drop(columns = ['Date'], axis = 1)
  scaler = MinMaxScaler(feature_range=(0,1))
  original = scaler.fit_transform(original)
  originalDf = pd.DataFrame(original)

  return original, originalDf

 ######################################################################
 ###Computes errors SMAPE, MAPE, RMSE, RMSLE
 ######################################################################
from typing import List
def eval_forecasts(pred_series: List[TimeSeries],
                   test_series: List[TimeSeries]) -> List[float]:

    #saving only SMAPE in the dataframe
    smapE = smape(TimeSeries.from_values(test_series), TimeSeries.from_values(pred_series))
    mapE = mape(TimeSeries.from_values(test_series), TimeSeries.from_values(pred_series))
    rmsE = rmse(TimeSeries.from_values(test_series), TimeSeries.from_values(pred_series))
    rmslE = rmsle(TimeSeries.from_values(test_series), TimeSeries.from_values(pred_series))

    return mapE,  smapE, rmsE, rmslE


 ######################################################################
## computes darts features for C1
 ######################################################################


from darts.utils.statistics import *


def dartsFeatures (data): # data= xdat_m4

  darts = pd.DataFrame()
  for ind in data.index:

    a = []
    xd = data.iloc[ind].dropna()
    xseries = TimeSeries.from_values(xd.values)
    a.append(xseries)
    dartsf1,dartsf2,dartsf3,dartsf4,dartsf5,dartsf6 = (stationarity_test_adf(a[0], maxlag=None, regression='c', autolag='AIC'))
    features = pd.DataFrame([dartsf1,dartsf2,dartsf3,dartsf4,dartsf6]).T
    darts = pd.concat([darts, features])

  return  darts

 ######################################################################
## computes catch features for C2
 ######################################################################
import pycatch22 as catch22

def catchFeatures (data): # data= xdat_m4

  ct = pd.DataFrame()
  for ind in data.index:
    a = []
    xd = data.iloc[ind].dropna()
    xseries = TimeSeries.from_values(xd.values)
    a.append(xseries)
    catchf1 = catch22.catch22_all(a[0].values())
    catch = (catchf1['values'])
    fr = pd.DataFrame(catch).T
    ct = pd.concat([ct, fr])

  return ct




 ######################################################################
 ### performs forecast without using series data
 ######################################################################
import pickle

def eval_model_woseries(horizon,model, Train, Test,PP,C):

    method = "PR2"
    M =  getModel(str(model))
    tag = C+PP+method+M+"Series"+str(indNum)
    model = model.fit(Train)

    with open(tag, 'wb') as f:
      pickle.dump(model, f)

    pred = model.predict(horizon)
    predv = pred.values()
    testv =  Test.values
    MAPE, SMAPE, RMSE, RMSLE = eval_forecasts(predv,testv)


    return SMAPE, predv, M, method, tag

 ######################################################################
 ### performs forecast with using series
 ######################################################################

def eval_model_wseries(horizon,model, Train, Series, Test, PP,C):

    method = "PR1"
    M =  getModel(str(model))
    Series = Series.dropna(axis=0)
    Series = Series.values
    Series = TimeSeries.from_values(Series)



    tag = C+PP+method+M+"Series"+str(indNum)
    model = model.fit(Train)
    with open(tag, 'wb') as f:
      pickle.dump(model, f)


    pred = model.predict(horizon, series= Series)
    predv = pred.values()
    testv =  Test.values
    MAPE, SMAPE, RMSE, RMSLE = eval_forecasts(predv,testv)

    return SMAPE, predv, M, method, tag

 #####################################################################

def processData(testFile,trainFile,metaData, indNum):

  wfytest, wfytrain, dfinfo = getData(testFile,trainFile,metaData )
  xdat_m4, xdaty_m4, merged_test, merged_train, DateTrain, DateTest, Date = mergeData(wfytest, wfytrain, dfinfo)
  actualTrain = xdat_m4.loc[indNum]
  actualTest = xdaty_m4.loc[indNum]
  season = len(actualTest)
  horizon = season

  return xdat_m4, xdaty_m4, merged_test, merged_train, DateTrain, DateTest, Date ,actualTrain, actualTest, season, horizon


########################################################
#Creating Covariates
########################################################

def Covariates(Test, PruneCluster, PP ): # PruneClusterDarts,  PruneClusterDartsUP,  PruneClusterDartsCatch

  lfs = []
  # login 1 times the horizon
  for i in range(1,horizon):
    lfs.append(-i)


  cov = []
  for i in PruneCluster.index:

    past_cov = PruneCluster.loc[i]
    past_covTest = xdaty_m4.loc[i]
    past_cov = past_cov.append(past_covTest)
    past_covS = TimeSeries.from_series(past_cov.values)
    cov.append(past_covS)

  mult_cov = concatenate([cov[i] for i, ind in enumerate(cov)], ignore_time_axis = True ) # axis can be an integer in (0, 1, 2) to denote (time, component, sample) or, alternatively, a string denoting the corresponding
                                                                                          # dimension of the underlying DataArray. Ref https://unit8co.github.io/darts/generated_api/darts.timeseries.html?highlight=concatenate#darts.timeseries.concatenate


  actualTrainCov = PruneCluster.loc[indNum]
  actualTrainCov = actualTrainCov[:-1]
  inCh = len(actualTrainCov)

  ################
  if PP == "PP1":
    cTS = actualTrainCov.append(Test)
    cTS = TimeSeries.from_values(cTS.values)

  if PP == "PP2":
    combineTS = actualTrainCov.append(Test)
    combineTS = TimeSeries.from_values(combineTS.values)
    combineTSMM = min_Max(combineTS)
    cTS = TimeSeries.from_values(combineTSMM.values())


  if PP == "PP3":
    combineTS = actualTrainCov.append(Test)
    combineTS = TimeSeries.from_values(combineTS.values)
    combineTSMM = min_Max(combineTS)
    combineTSMM = TimeSeries.from_values(combineTSMM.values())
    combineTSNorm = normalize(combineTSMM.values())
    cTS = TimeSeries.from_values(combineTSNorm)


  if PP == "PP4":
    combineTS = actualTrainCov.append(Test)
    combineTS = TimeSeries.from_values(combineTS.values)
    combineTSNorm = normalize(combineTS.values())
    cTS = TimeSeries.from_values(combineTSNorm)


#######################################################
  CTS = cTS[-horizon:]
  Tr = cTS[:-horizon]

  return cTS, CTS, Tr, mult_cov, lfs



#####################################################################

from sklearn.preprocessing import normalize

 ######################################################################
 ### c1= no processing, c2 = darts, c3 = catch (library catch-22)
 ######################################################################

def processCluster(clusterNum , xdat_m4, cat):

  global PruneClusterUP
  global PruneClusterDarts
  global PruneClusterCatch


  if cat == "C1":

    rawClusterData = makeCluster(clusterNum , xdat_m4, cat)
    xdat_m4[cat] = rawClusterData[cat]
    grp= clusterGrp(rawClusterData, clusterNum, cat )
    clusterScaleUP  = getClusterforScaling(clusterNum, grp)
    Cluster,clusterLength  = getClusterLength(grp[clusterScaleUP])
    PruneClusterUP = pruneCluster(Cluster,clusterLength, cat)

    return  PruneClusterUP



  elif cat == "C2":

    dfqeat = dartsFeatures (xdat_m4)
    xdarts = xdat_m4.copy()
    xdarts = xdarts.append(dfqeat, ignore_index=True)
    xdarts.columns = xdarts.columns.map(str)
    ClusterDataDarts = makeCluster(clusterNum , xdarts, "Darts")
    xdat_m4['Darts'] = xdarts['Darts']
    grpdarts= clusterGrp(ClusterDataDarts, clusterNum, "Darts")
    clusterScaleDarts  = getClusterforScaling(clusterNum, grpdarts)
    ClusterDarts,clusterLengthDarts  = getClusterLength(grpdarts[clusterScaleDarts])
    PruneClusterDarts = pruneCluster(ClusterDarts,clusterLengthDarts,"Darts")


    return  PruneClusterDarts



  elif cat == "C3":
    cfqeat = catchFeatures (xdat_m4)
    xcatch = xdat_m4.copy()
    xcatch = xcatch.append(cfqeat, ignore_index=True)
    xcatch.columns = xcatch.columns.map(str)
    ClusterCatchData = makeCluster(clusterNum , xcatch, "Catch")
    xdat_m4['Catch'] = ClusterCatchData['Catch']
    grpcatch= clusterGrp(ClusterCatchData, clusterNum, "Catch")
    clusterScaleCatch  = getClusterforScaling(clusterNum, grpcatch)
    ClusterCatch,clusterLengthCatch = getClusterLength(grpcatch[clusterScaleCatch])
    PruneClusterCatch = pruneCluster(ClusterCatch,clusterLengthCatch, "Catch")


    return   PruneClusterCatch





#####################################################################

def createRecord():
  ResultsDF = pd.DataFrame(columns=['TrainSeries','TestSeries','PredictSeries','SMAPE','Model','Category','Series'])
  TrainSeriesLS = []
  TestSeriesLS =[]
  PredictSeriesLS = []
  ELS = []
  MLS = []
  CLS = []
  PRLS = []
  PPLS = []
  indNumLS = []
  tagLS = []
  return ResultsDF,  TrainSeriesLS, TestSeriesLS, PredictSeriesLS, ELS, MLS, CLS, PRLS, PPLS, indNumLS, tagLS

#####################################################################


def push(actualTrain, actualTest, lP,  SMAPE, label, category, method,pp, indNum, tag):
  TrainSeriesLS.append(actualTrain.values)
  TestSeriesLS.append(actualTest.values)
  PredictSeriesLS.append(lP)
  ELS.append(SMAPE)
  MLS.append(label)
  CLS.append(category)
  PRLS.append(method)
  PPLS.append(pp)
  indNumLS.append(indNum)
  tagLS.append(tag)


######################################################################
## USed for labelling M1, M2, M3 ....
######################################################################

def getModel(keyInput):

    ModelRef = {

    "LinearRegression(n_jobs=-1)": "M1",
    "LinearRegression()": "M2",
    "LGBMRegressor()": "M3",
    "RandomForestRegressor()" : "M4"

    }


    for (key, value) in set(ModelRef.items()):
      if key  == keyInput:
        return (value)


##########################################################

def downloadCSV(TrainSeriesLS,TestSeriesLS,PredictSeriesLS,ELS, MLS, CLS, PRLS, PPLS,indNumLS ):

  ResultsDF['TrainSeries'] = TrainSeriesLS
  ResultsDF['TestSeries'] = TestSeriesLS
  ResultsDF['PredictSeries'] = PredictSeriesLS
  ResultsDF['SMAPE'] = ELS
  ResultsDF['Model'] = MLS
  ResultsDF['Category'] =CLS
  ResultsDF['PR']= PRLS
  ResultsDF['PP'] = PPLS
  ResultsDF['Series'] = indNumLS
  ResultsDF['Tag'] = tagLS

  from google.colab import files
  ResultsDF.to_csv('ResultsM4-Weekly.csv')
  files.download('ResultsM4-Weekly.csv')


##########################################################

def Visualize(PredictSeriesLS,MLS, tagLS):
  for indNum,i,y,x in zip (indNumLS, PredictSeriesLS, MLS, tagLS):

      if (y == 'M1'):
        plt.plot(i,label = x, color='r', linestyle='--')
        #plt.plot(i,  color='r', linestyle='-')
      if (y == 'M2'):
        plt.plot(i, label = x, color='b', linestyle='--')
      if (y == 'M3'):
        plt.plot(i, label = x, color='g', linestyle='--')
      if (y == 'M4'):
        plt.plot(i,label = x, color='orange', linestyle='--')

  plt.plot(actualTest, label = "Target", color='Black',  linewidth = '3.5', linestyle='--')
  plt.title(indNum)
  plt.rcParams["figure.figsize"] = (20,10)
  images_dir = '/content/gdrive/My Drive/NTSA-project/NonNeuralTSA/Images/M4-Weekly'

  plt.legend()
  plt.savefig(f"{images_dir}/Series{indNum}.png")
  plt.show()



##########################################################
def runRegressionModel(output_chunk_length, lags, lags_past_covariates, lags_future_covariates, mult_cov):

  regr_model = RegressionModel(
                              output_chunk_length = output_chunk_length , #Number of time steps predicted at once (per chunk) by the internal model. It is not the same as forecast horizon n
                              #lags input should be flexible not hardcoded
                              lags = lags,
                              lags_past_covariates = lags,
                              lags_future_covariates = lags_past_covariates
                              )

  regr_model.fit(combineTS, past_covariates= mult_cov,future_covariates= mult_cov)
  pred = regr_model.predict(n= horizon )
  PR = "PR1"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)
  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum, tag )

              #####################################################################
              # [C1 C2 C3] [PP1 PP2 PP3] [PR2]  [E1] [M1] [D1]
              #####################################################################

  pred = regr_model.predict(n= horizon, series = combineTS[0:-13] )
  PR = "PR2"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)


  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum, tag )

#####################################################################


def runLinearRegressionModel(output_chunk_length, lags, lags_past_covariates, mult_cov):

  regr_model = LinearRegressionModel(
                              output_chunk_length = output_chunk_length, #Number of time steps predicted at once (per chunk) by the internal model. It is not the same as forecast horizon n
                              lags = lags, #Lagged target series values used to predict the next time step/s
                              lags_past_covariates = lags_past_covariates

                                )

  regr_model.fit(combineTS,past_covariates= mult_cov)
  CTS = combineTS[-13:]
  pred = regr_model.predict(n= horizon)
  PR = "PR1"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)

  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum, tag )

              #####################################################################
              # [C1 C2 C3] [PP1 PP2 PP3] [PR2]  [E1] [M2] [D1]
              #####################################################################
  pred = regr_model.predict(n= horizon, series = combineTS[0:-13] )
  PR = "PR2"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)

  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum, tag )



def runRandomForest(output_chunk_length, lags, lags_past_covariates, mult_cov):
                #####################################################################
              # [C1 C2 C3] [PP1 PP2 PP3] [PR1]  [E1] [M3] [D1]
              #####################################################################

  RF_model = RandomForest(     output_chunk_length = output_chunk_length, #Number of time steps predicted at once (per chunk) by the internal model. It is not the same as forecast horizon n
                              lags = lags, #Lagged target series values used to predict the next time step/s
                              lags_past_covariates = lags_past_covariates
                              )

  RF_model.fit(combineTS[:-13], past_covariates= mult_cov)
  pred =  RF_model.predict(horizon, series = combineTS[0:-13])
  PR = "PR1"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)

  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum,tag )

              #####################################################################
              # [C1 C2 C3] [PP1 PP2 PP3] [PR2]  [E1] [M3] [D1]
              #####################################################################
  pred = RF_model.predict(n= horizon, series = combineTS[0:-13] )
  PR = "PR2"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)

  with open(tag, 'wb') as f:
    pickle.dump(model, f)

  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum,tag )

def runLightGBM(horizon, lags_past_covariates,mult_cov):

              #####################################################################
              # [C1 C2 C3] [PP1 PP2 PP3] [PR1]  [E1] [M4] [D1]
              #####################################################################
  LightGBM_model = LightGBMModel(lags=None,
                              lags_past_covariates= lfs,
                              )
  LightGBM_model.fit(combineTS[:-13],past_covariates= mult_cov)


  pred =  LightGBM_model.predict(horizon, series = combineTS[0:-13])
  PR = "PR1"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)




  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum,tag )


              #####################################################################
              # [C1 C2 C3] [PP1 PP2 PP3] [PR2]  [E1] [M4] [D1]
              #####################################################################

  pred =  LightGBM_model.predict(horizon, series = combineTS[0:-13])
  PR = "PR2"
  lC = CTS.values()
  forecast = pred.values()
  smapE = eval_forecasts(lC, forecast)
  E = np.round(smapE[1],2)
  tag = C+PP+PR+M+"Series"+str(indNum)
  with open(tag, 'wb') as f:
    pickle.dump(model, f)

  push(actualTrain, actualTest, forecast,  E, M, C, PR,PP , indNum, tag)




################################################################################
## adding timestamps for resampling
################################################################################

import datetime

#freq =

#  "H" for hourly
#  "D" for daily
#  "W" for Weekly
#  "M" for monthly
#  "Q" for quarterly
#  "Y" for yearly
# only made for M4 dataset

# two filters

def timeTag(data, freq, Trainindex):
  #matching time stamp
  # calculating number of seasons to add (week, month, days etc)

  data['StartingDate'] = pd.to_datetime(data['StartingDate'])      #, format="%d-%m-%Y %H:%M")
  weeks = (len(data.iloc[Trainindex].dropna()))
  weeks = weeks-6 # number of weeks, will be used for pruning in the next stage
  seriesStartDate = data['StartingDate'].iloc[Trainindex]
  times = pd.date_range(start=pd.Timestamp(seriesStartDate), periods = weeks, freq = freq)
  pd_series = pd.Series(range(weeks), index = times) #time index series
  ts = TimeSeries.from_series(pd_series)
  returnSeries = data.iloc[Trainindex]
  returnSeries = returnSeries[6:].T.dropna()
  newFrame = pd.DataFrame()
  newFrame['values'] = returnSeries.values
  newFrame['Week'] = pd_series.index

  return newFrame, weeks, pd_series


def timeTagTest(data, freq, Trainindex, date):
  #matching time stamp
  # calculating number of seasons to add (week, month, days etc)

  data['StartingDate'] = pd.to_datetime(date)      #, format="%d-%m-%Y %H:%M")
  weeks = (len(data.iloc[Trainindex].dropna()))
  weeks = weeks-6 # number of weeks, will be used for pruning in the next stage
  seriesStartDate = data['StartingDate'].iloc[Trainindex]
  times = pd.date_range(start=pd.Timestamp(seriesStartDate), periods = weeks, freq = freq)
  pd_series = pd.Series(range(weeks), index = times) #time index series
  ts = TimeSeries.from_series(pd_series)
  returnSeries = data.iloc[Trainindex]
  returnSeries = returnSeries[6:].T.dropna()
  newFrame = pd.DataFrame()
  newFrame['values'] = returnSeries.values
  newFrame['Week'] = pd_series.index

  return newFrame, weeks, pd_series


def pruneTData(startDate, endDate, merged_train, weeks, seriesNF):
  #prune to series length
  res = []
  ind = []
  for i in merged_train.index:

    seriesNF, val , ts = timeTag(merged_train, "W", i)
    filtered_df2 = seriesNF[(seriesNF['Week'] == startDate)  ]
    filtered_df2 = filtered_df2[:weeks]

    if(filtered_df2.empty):
      pass
    else:
      #res.append(filtered_df2['values'].T)
      ind.append(i)
      res.append(filtered_df2['values'])

  newData = pd.DataFrame()
  newData['SeriesID'] = ind
  #newData['Values']= res
  newData.set_index(['SeriesID'])
  nd = merged_train.iloc[(newData['SeriesID'].values)]
  nd = nd.iloc[:, 6 : weeks+6].dropna(axis=0)


  return nd

xdat_m4, xdaty_m4, merged_test, merged_train, DateTrain, DateTest, Date ,actualTrain, actualTest, season, horizon = processData( testFile, trainFile, metaData, indNum )
seriesNF, weeks, ts = timeTag(merged_train, "W", 5)

#freq =

#  "H" for hourly
#  "D" for daily
#  "W" for Weekly
#  "M" for monthly
#  "Q" for quarterly
#  "Y" for yearly
# only made for M4 dataset

endDate = seriesNF['Week'].iloc[-1]
startDate = seriesNF['Week'][0]
resampledDataFrame = pruneTData(startDate, endDate, merged_train, weeks, seriesNF)
resampledDataFrame  = pd.DataFrame(resampledDataFrame )
date_time_var = ts.index
dateCol = (date_time_var.strftime('%Y-%m-%d'))
actualTrain = actualTrain[~np.isnan(actualTrain)]
actualTrain.index = dateCol
tstStrtDate = pd.to_datetime(ts.index[-1])
d = datetime.timedelta(days = 7)    #7 for W, 1 for D, 30 for M, 120 for Q 365 for Y
strDate =  tstStrtDate  + d


seriesNFTest, weeksTest, tsTest = timeTagTest(merged_test, "W", 5, strDate )
seriesNFTest.index =seriesNFTest['Week']
seriesNFTest = seriesNFTest.drop(columns=['Week']) #test data with time index
seriesNF.index =seriesNF['Week']
seriesNF = seriesNF.drop(columns=['Week'])  #train data with time index

ind = resampledDataFrame.index


s = ts.index   # time index for actualtrain data
t = seriesNFTest.index  # time index for actualtest data


tts = s.append(t) # combines time index for train and test

endDate

"""# Execute Code"""

resampled = xdat_m4.iloc[resampledDataFrame.index]
resampled = resampledDataFrame.iloc[:,  : weeks+6].dropna(axis=0)

seriesNFTest
# put time stamp in the first column replace v2, v3 ....
# why nans come back after dropping
#Output of preprocessing

resampled

#for indNum in range(len(trainFile)):
for indNum in range(3):
  from sklearn.preprocessing import normalize
  ResultsDF,  TrainSeriesLS, TestSeriesLS, PredictSeriesLS, ELS, MLS, CLS, PRLS , PPLS , indNumLS, tagLS = createRecord()
  xdat_m4, xdaty_m4, merged_test, merged_train, DateTrain, DateTest, Date ,actualTrain1, actualTest, season, horizon = processData( testFile, trainFile, metaData, indNum )
  Cls = ["C0",  "C1", "C2", "C3" ] # raw,simpleK-means, darts, catch
  PPls = ["PP1", "PP2", "PP3", "PP4"] # unprocessed, minmax, minmaxnormalized, normalized


  NormPP = actualTrain
  NormData = NormPP.dropna(axis = 0)

  MinMaxTrain = min_MaxC0(NormData)
  mmt = MinMaxTrain.values.reshape(-1,1)
  MMTN = normalize(mmt, axis = 0, norm='l2')

  NormData = TimeSeries.from_values(NormData.values)
  NormTrain = normalize(NormData.values(), axis = 0, norm='l2')

  train = actualTrain
  test = actualTest

  for model in models:
    for PP in PPls:
      for C in Cls:


            if PP == "PP1":
              train = pd.DataFrame(train)
              train = train.dropna(axis=0)
              trainpp = TimeSeries.from_values(train.values)


            if PP == "PP2":
              trainmm = MinMaxTrain
              trainmm = train.dropna(axis=0)
              trainpp = TimeSeries.from_values(trainmm.values)

            if PP == "PP3":
              trainpp = MMTN
              trainpp = TimeSeries.from_values(trainpp)

            if PP == "PP4":
              trainpp = NormTrain
              trainpp = TimeSeries.from_values(trainpp)


            #####################################################################
            # [C0] [PP1 PP2 PP3] [PR1 PR2]  [E1] [M1 M2 M3 M4] [D1]
            #####################################################################

            if C == "C0":

              E, forecast, M , PR , tag = eval_model_woseries(horizon, model(lags = horizon), trainpp, test, PP, C)  # input variations trainpp: actualTrain MinMaxTrain, normalizedTrain
              #print(actualTrain, actualTest, forecast,  E, M, C, PR,PP)
              push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum, tag)


              E, forecast, M ,PR , tag = eval_model_wseries(horizon, model(lags = horizon), trainpp, train, test, PP, C )  # input variations trainpp: actualTrain MinMaxTrain, normalizedTrain
              #print(actualTrain, actualTest, forecast,  E, M, C, PR,PP)
              push(actualTrain, actualTest, forecast,  E, M, C, PR,PP, indNum, tag )


            else:

                #PruneCluster =  processCluster(clusterNum ,xdat_m4, C) # need PruneCluster for creating covariates, does not return anything for c0
                PruneCluster= resampled
                #PruneCluster = (PruneCluster.iloc[:, :-1])
                combineTS, CTS, Tr, mult_cov, lfs = Covariates(actualTest, PruneCluster, PP) # PruneClusterDarts,  PruneClusterDartsUP,  PruneClusterDartsCatch
                mod =  model(lags=horizon)
                M = getModel(str(mod))

                if(M == "M1"):
                  runRegressionModel(horizon,lfs,lfs,lfs, mult_cov)

                if(M == "M2"):
                  runLinearRegressionModel(horizon,lfs,lfs, mult_cov)

                if M == "M3":
                  runRandomForest(horizon,lfs,lfs, mult_cov)

                if M == "M4":
                  runLightGBM(horizon, lfs, mult_cov)

downloadCSV(TrainSeriesLS,TestSeriesLS,PredictSeriesLS,ELS, MLS, CLS, PRLS, PPLS,indNumLS )
from google.colab import drive
drive.mount('/content/gdrive')
!cp -r "/content" "/content/gdrive/My Drive/NTSA-project/NonNeuralTSA/Models/M4-Weekly"
Visualize(PredictSeriesLS,MLS,tagLS)

PredictSeriesLS

Visualize(PredictSeriesLS,MLS,tagLS)